{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19523,"status":"ok","timestamp":1686980972218,"user":{"displayName":"ANKEM HEMA SRIKAR / 학생 / 항공우주공학과 ­","userId":"08215093842889631142"},"user_tz":-540},"id":"M00vAJvZMKSw","outputId":"7d99d233-5c75-48c9-9e0b-b0248aab3902"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1686980972219,"user":{"displayName":"ANKEM HEMA SRIKAR / 학생 / 항공우주공학과 ­","userId":"08215093842889631142"},"user_tz":-540},"id":"kWKN9KqFiIDF","outputId":"42e0ce07-8a01-4b42-c6af-ec2bb67b715a"},"outputs":[],"source":["# chage path as you want\n","%cd /content/drive/MyDrive/Colab Notebooks"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":519,"status":"ok","timestamp":1686980972733,"user":{"displayName":"ANKEM HEMA SRIKAR / 학생 / 항공우주공학과 ­","userId":"08215093842889631142"},"user_tz":-540},"id":"3cn7ReSLI8KI"},"outputs":[],"source":["from copy import deepcopy\n","from IPython.display import display, HTML\n","import math\n","from math import sin, cos\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.animation as animation\n","import numpy as np\n","from numpy.random import rand\n","import pickle\n","import time\n","from tqdm import tqdm\n","\n","PI = math.pi\n","\n","class ThrowingEnvironmentBase():\n","    def __init__(self, student_id, q_size):\n","\n","        # Kinematic / Dynamic parameters\n","        self.bound = [(-PI/2, PI/2), (-PI/2, PI/2), (-3*PI, 3*PI), (-3*PI, 3*PI)] # lower/upper bounds for theta1, theta2, omega1, omega2\n","        self.length = [1.0, 1.0] # L1, L2\n","        self.gravity = 9.8\n","        self.trashcan_position = [6.0, -4.0]\n","        self.trashcan_height = 2.0\n","        self.trashcan_radius = 3\n","        self.robot_mass = [1.0, 1.0]\n","        self.trash_mass =  0.5\n","        self.trash_color = 'lightgray'\n","        self.torque = 10.0\n","        self.dt = 0.01\n","        self.action_list = [[1,1,0],[1,-1,0],[-1,1,0],[-1,-1,0],[1,1,1],[1,-1,1],[-1,1,1],[-1,-1,1]]\n","                            # upper arm, lower arm, holding trash = 0 / released = 1\n","\n","        # Reinforcement Learning parameters\n","        self.alpha = 0.1\n","        self.gamma = 0.99\n","        self.epsilon = 0.1\n","\n","        self.fail_reward = -100.0\n","        self.success_reward = 1000.0\n","        self.reward_per_step = 0\n","\n","        self.state = [0.0, 0.0, 0.0, 0.0] # theta1, theta2, omega1, omega2\n","        self.trash_state = [0.0, -2.0, 0.0, 0.0] # x, y, vx, vy\n","        self.is_released = 0 # 1 : released\n","        self.terminal = 0 # 1 : terminate\n","\n","        # Class information\n","        self.epoch = 0\n","        self.step = 0\n","        self.step_limit = 500\n","        self.step_per_epoch = []\n","        self.q_max_per_epoch = []\n","        self.q_avg_per_epoch = []\n","        self.success_number = 0\n","\n","        # Input information\n","        self.q_size = q_size\n","        self.q_table = np.zeros(q_size + [8])\n","        self.student_id = student_id\n","        self.seed = int(student_id.split('-')[1])\n","        np.random.seed(self.seed)\n","        print(\"Environment Initialized\")\n","\n","    def state_mod(self): # function to make thetas in the range of (-PI, PI)\n","        for i in range(2):\n","            n = int(self.state[i] / (2 * PI))\n","            self.state[i] -= n * (2 * PI)\n","            if self.state[i] > PI:\n","                self.state[i] -= 2 * PI\n","            if self.state[i] < - PI:\n","                self.state[i] += 2 * PI\n","            if self.state[i] > PI:\n","                self.state[i] -= 2 * PI\n","            if self.state[i] < - PI:\n","                self.state[i] += 2 * PI\n","\n","    def end_position(self): # end-effector position\n","        t = self.state[0:2]\n","        l = self.length\n","        return [sin(t[0])*l[0]+sin(t[0] + t[1])*l[1], -cos(t[0])*l[0]-cos(t[0] + t[1])*l[1]]\n","\n","    def end_velocity(self): # end-effector velocity\n","        t = self.state[0:2]\n","        w = self.state[2:4]\n","        l = self.length\n","        return [l[0]*cos(t[0])*w[0]+l[1]*cos(t[0]+t[1])*(w[0]+w[1]), l[0]*sin(t[0])*w[0]+l[1]*sin(t[0]+t[1])*(w[0]+w[1])]\n","\n","    def reset(self, fixed_state = None): # restart simulation / training\n","        self.state = [(rand()-0.5)*(self.bound[0][1]-self.bound[0][0])/3, (rand()-0.5)*(self.bound[1][1]-self.bound[1][0])/3, 0, 0]\n","        if fixed_state is not None:\n","            self.state = fixed_state\n","        self.trash_state = deepcopy(self.end_position()+self.end_velocity())\n","        self.is_released = 0\n","        self.terminal = 0\n","        self.step = 0\n","        self.trash_color = 'lightgray'\n","\n","    def action_sample_idx(self): # sample random action index\n","        pass\n","\n","    def epsilon_greedy_action_idx(self, state_idx): # epsilon greedy action, TODO\n","        pass\n","\n","    def discretize(self, state): # make continuous state to discrete state, and return its index\n","        idx = np.zeros_like(state, dtype=int)\n","        for i in range(len(state)):\n","            idx[i] = min(max(math.floor((state[i]-self.bound[i][0]) / (self.bound[i][1]-self.bound[i][0]) * self.q_size[i]),0), self.q_size[i]-1)\n","        return tuple(idx)\n","\n","    def derivs(self, action): # dynamics of robot arm and trash\n","        dydx = np.zeros_like(self.state)\n","        M1, M2 = self.robot_mass[0], self.robot_mass[1]\n","        M = self.trash_mass\n","        if self.is_released:\n","            M = 0\n","        L1, L2 = self.length\n","        G = self.gravity\n","        theta1, theta2 = self.state[0], self.state[0] + self.state[1]\n","        omega1, omega2 = self.state[2], self.state[2] + self.state[3]\n","        dydx[0] = omega1\n","        dydx[1] = omega2\n","        delta = theta2 - theta1\n","        den1 = (M+M1+M2) * L1 - (M+M2) * L1 * cos(delta) * cos(delta)\n","        dydx[2] = (((M+M2) * L1 * omega1 * omega1 * sin(delta) * cos(delta)\n","                    + (M+M2) * G * sin(theta2) * cos(delta)\n","                    + (M+M2) * L2 * omega2 * omega2 * sin(delta)\n","                    - (M+M1+M2) * G * sin(theta1))\n","                / den1)\n","        den2 = (L2/L1) * den1\n","        dydx[3] = ((- (M+M2) * L2 * omega2 * omega2 * sin(delta) * cos(delta)\n","                    + (M+M1+M2) * G * sin(theta1) * cos(delta)\n","                    - (M+M1+M2) * L1 * omega1 * omega1 * sin(delta)\n","                    - (M+M1+M2) * G * sin(theta2))\n","                / den2)\n","        dydx[1] -= dydx[0]\n","        dydx[3] -= dydx[2]\n","        dydx[2] += self.torque * action[0]\n","        dydx[3] += self.torque * action[1]\n","        return dydx\n","\n","    def next_state(self, action): # find (next_state, instant reward, whether the trash has been released or not, whether the simulation should be terminated or not) using current state and input action\n","        assert len(action)==3, \"action space dim should be 3\"\n","        new_state = self.state + self.derivs(action) * self.dt\n","        self.state = new_state\n","        self.state_mod()\n","        if not self.is_released:\n","            self.trash_state = deepcopy(self.end_position()+self.end_velocity())\n","        self.is_released = self.is_released + action[2]\n","        token = 0\n","        for i in range(len(self.state)):\n","            if self.state[i]<self.bound[i][0] or self.state[i]>self.bound[i][1]:\n","                token = i\n","                break\n","        if token or self.is_released or self.step > self.step_limit:\n","            self.terminal = 1\n","        return self.state, deepcopy(self.reward()), self.is_released, self.terminal\n","\n","    def reward(self): # reward for current state and action\n","        if not self.is_released:\n","            if self.terminal:\n","                return self.fail_reward\n","            return self.reward_per_step\n","\n","        trashcan_upperleft = [self.trashcan_position[0]-self.trashcan_radius, self.trashcan_position[1]+self.trashcan_height]\n","        trashcan_upperright = [self.trashcan_position[0]+self.trashcan_radius, self.trashcan_position[1]+self.trashcan_height]\n","        trash_position = self.trash_state[0:2]\n","        trash_velocity = self.trash_state[2:4]\n","        if trash_velocity[0] <= 0:\n","            return self.fail_reward\n","        time_l = (trashcan_upperleft[0] - trash_position[0]) / trash_velocity[0]\n","        trash_left = trash_position[1] + trash_velocity[1] * time_l - 1/2 * self.gravity * time_l ** 2\n","        if trash_left < trashcan_upperleft[1]:\n","            return self.fail_reward\n","        time_r = (trashcan_upperright[0] - trash_position[0]) / trash_velocity[0]\n","        trash_right = trash_position[1] + trash_velocity[1] * time_r - 1/2 * self.gravity * time_r ** 2\n","        if trash_right > trashcan_upperright[1]:\n","            return self.fail_reward\n","        time_inside = (trash_velocity[1] + math.sqrt(trash_velocity[1]**2 - 2 * self.gravity * (trashcan_upperleft[1]-trash_position[1]))) / self.gravity\n","        reward_inside = self.success_reward * (2 - abs(time_inside * trash_velocity[0] + trash_position[0] - self.trashcan_position[0]) / self.trashcan_radius)\n","        return reward_inside\n","\n","    def train_step(self): # train one step, TODO\n","        pass\n","\n","    def train(self, train_n): # training\n","        for _ in tqdm(range(train_n)):\n","            self.reset()\n","            self.epoch += 1\n","            while not self.is_released:\n","                terminal = self.train_step()\n","                if terminal:\n","                    break\n","            self.step_per_epoch.append(self.step)\n","            self.q_max_per_epoch.append(np.max(self.q_table))\n","            self.q_avg_per_epoch.append(np.average(self.q_table))\n","            if self.reward()>0:\n","                self.success_number += 1\n","\n","    def visualization_step(self): # visualize one step\n","        self.step += 1\n","        state_idx = self.discretize(self.state)\n","        A = self.q_table[state_idx]\n","        action = self.action_list[np.random.choice(np.where(A == A.max())[0])]\n","        if not self.is_released:\n","            _, _, _, _ = self.next_state(action)\n","\n","        return self.terminal, deepcopy(self.reward())\n","\n","    def visualization(self): # visualize, when released or terminated - the robot stops; when released - the trash moves in projectile motion\n","        if (not self.is_released) and (self.terminal):\n","            return self.terminal, deepcopy(self.reward())\n","\n","        terminal, reward, = self.visualization_step()\n","        if self.is_released:\n","            if ((self.trash_state[0] >= (self.trashcan_position[0] - self.trashcan_radius)) and ((self.trashcan_position[0] + self.trashcan_radius) >=self.trash_state[0]) and \\\n","                (self.trash_state[1]>=self.trashcan_position[1]) and ((self.trashcan_position[1] + self.trashcan_height) >= self.trash_state[1])):\n","                previous_velocity_x = self.trash_state[2]\n","                previous_velocity_y = self.trash_state[3] + self.gravity * self.dt\n","                previous_position_x = self.trash_state[0] - previous_velocity_x * self.dt\n","                previous_position_y = self.trash_state[1] - previous_velocity_y * self.dt + 1/2 * self.gravity * self.dt **2\n","                left_height = (previous_position_y - self.trash_state[1]) / (previous_position_x - self.trash_state[0]) * \\\n","                                (self.trashcan_position[0] - self.trashcan_radius - self.trash_state[0]) + self.trash_state[1]\n","                if left_height < self.trashcan_position[1] + self.trashcan_height:\n","                    self.trash_state[2] = - self.trash_state[2]\n","                else:\n","                    self.trash_color = 'green'\n","\n","            if (self.trash_color == 'lightgray'):\n","                self.trash_state[0] += self.dt * self.trash_state[2]\n","                self.trash_state[1] += self.dt * self.trash_state[3] - 1/2 * self.gravity * self.dt ** 2\n","                self.trash_state[3] += - self.gravity * self.dt\n","\n","        return terminal, reward\n","\n","    def summary(self): # training information\n","        fig = plt.figure(figsize=(10,12))\n","        ax1 = fig.add_subplot(311)\n","        ax1.plot(self.step_per_epoch)\n","        ax1.set_title(\"step per epoch\")\n","        ax2 = fig.add_subplot(312)\n","        ax2.plot(self.q_max_per_epoch)\n","        ax2.set_title(\"Q_max per epoch\")\n","        ax3 = fig.add_subplot(313)\n","        ax3.plot(self.q_avg_per_epoch)\n","        ax3.set_title(\"Q_avg per epoch\")\n","        print(\"success rate : \",str(self.success_number),\"/\",str(self.epoch))\n","\n","    def plot_state(self): # plot current state\n","        fig = plt.figure(figsize=(7,5))\n","        ax = fig.add_subplot(111)\n","        ax.set_xlabel('X')\n","        ax.set_ylabel('Y')\n","        ax.axis('equal')\n","        ax.axis([-3, 11, -5, 5])\n","\n","        x = [0, sin(self.state[0])*self.length[0], sin(self.state[0])*self.length[0]+sin(self.state[0] + self.state[1])*self.length[1]]\n","        y = [0, -cos(self.state[0])*self.length[0], -cos(self.state[0])*self.length[0]-cos(self.state[0] + self.state[1])*self.length[1]]\n","        ax.plot(x, y, color = 'b', linewidth = 5)\n","        ax.add_patch(plt.Circle((self.trash_state[0],self.trash_state[1]),0.2,edgecolor = 'deeppink', facecolor = 'lightgray', fill = True))\n","        trashcan_x = [self.trashcan_position[0]-self.trashcan_radius, self.trashcan_position[0]-self.trashcan_radius,\n","                        self.trashcan_position[0]+self.trashcan_radius, self.trashcan_position[0]+self.trashcan_radius]\n","        trashcan_y = [self.trashcan_position[1]+self.trashcan_height, self.trashcan_position[1],\n","                        self.trashcan_position[1], self.trashcan_position[1]+self.trashcan_height]\n","        ax.plot(trashcan_x, trashcan_y, color = 'r', linewidth = 5)\n","        ax.text(0.05, 0.9, 'Current State', transform = ax.transAxes)\n","\n","        plt.show()\n","\n","    def plot_simulate(self, duration, reset = True, is_display = True, save_mp4 = True, save_name = 'simulation'): \n","        if reset:\n","            self.reset()\n","        saved_state = self.state\n","\n","        fig = plt.figure(figsize=(7,5))\n","        ax = fig.add_subplot(autoscale_on=False, xlim=(-3, 11), ylim=(-5, 5))\n","        ax.set_xlabel('X')\n","        ax.set_ylabel('Y')\n","        ax.set_aspect('equal')\n","\n","        def animate(i):\n","            plt.cla()\n","            terminal, reward = self.visualization()\n","            ax.set_xlabel('X')\n","            ax.set_ylabel('Y')\n","            ax.axis('equal')\n","            ax.axis([-3, 11, -5, 5])\n","            # print(self.state)\n","            x = [0, sin(self.state[0])*self.length[0], sin(self.state[0])*self.length[0]+sin(self.state[0] + self.state[1])*self.length[1]]\n","            y = [0, -cos(self.state[0])*self.length[0], -cos(self.state[0])*self.length[0]-cos(self.state[0] + self.state[1])*self.length[1]]\n","            ax.plot(x, y, color = 'b', linewidth = 5)\n","            ax.add_patch(plt.Circle((self.trash_state[0],self.trash_state[1]),0.2,edgecolor = 'green', facecolor = self.trash_color, fill = True))\n","            trashcan_x = [self.trashcan_position[0]-self.trashcan_radius, self.trashcan_position[0]-self.trashcan_radius,\n","                          self.trashcan_position[0]+self.trashcan_radius, self.trashcan_position[0]+self.trashcan_radius]\n","            trashcan_y = [self.trashcan_position[1]+self.trashcan_height, self.trashcan_position[1],\n","                          self.trashcan_position[1], self.trashcan_position[1]+self.trashcan_height]\n","            ax.plot(trashcan_x, trashcan_y, color = 'r', linewidth = 5)\n","            if self.is_released:\n","                ax.text(0.05, 0.9, 'time = %.1fs, released'%(i*self.dt),transform = ax.transAxes)\n","            elif terminal:\n","                ax.text(0.05, 0.9, 'time = %.1fs, terminated'%(i*self.dt),transform = ax.transAxes)\n","            else:\n","                ax.text(0.05, 0.9, 'time = %.1fs'%(i*self.dt),transform = ax.transAxes)\n","            if reward > 0:\n","                ax.text(0.05, 0.05, 'success', transform = ax.transAxes)\n","\n","            return fig,\n","\n","        if is_display:\n","            self.reset()\n","            self.state = saved_state\n","            ani = animation.FuncAnimation(fig, animate, repeat_delay = 500,\n","                                        frames=int(duration/self.dt), interval=self.dt * 1000, blit=True)\n","            display(HTML(ani.to_jshtml()))\n","\n","        if save_mp4:\n","            self.reset()\n","            self.state = saved_state\n","            ani2 = animation.FuncAnimation(fig, animate, repeat_delay = 500,\n","                                            frames=int(duration/self.dt), interval=self.dt * 1000, blit=True)\n","            ani2.save('RL_'+save_name+'.mp4')\n","\n","    def load_model(self, filename): # load your model using .pkl\n","        with open(filename, 'rb') as f:\n","            data = pickle.load(f)\n","            self.q_table = data[0]\n","            self.step_per_epoch = data[1]\n","            self.q_max_per_epoch = data[2]\n","            self.q_avg_per_epoch = data[3]\n","            self.success_number = data[4]\n","            self.epoch = data[5]\n","            self.student_id = data[6]\n","\n","    def save_model(self, filename): # save your model using .pkl\n","        Q_TABLE = self.q_table\n","        STEP_PER_EPOCH = self.step_per_epoch\n","        Q_MAX_PER_EPOCH = self.q_max_per_epoch\n","        Q_AVG_PER_EPOCH = self.q_avg_per_epoch\n","        SUCCESS_NUMBER = self.success_number\n","        EPOCH = self.epoch\n","        STUDENT_ID = self.student_id\n","        OBJECTS = [Q_TABLE, STEP_PER_EPOCH, Q_MAX_PER_EPOCH, Q_AVG_PER_EPOCH, SUCCESS_NUMBER, EPOCH, STUDENT_ID]\n","        with open(filename, 'wb') as f:\n","            pickle.dump(OBJECTS, f, pickle.HIGHEST_PROTOCOL)\n","\n","    def submit(self): # create your submit file\n","        filename = self.student_id + '.pkl'\n","        self.save_model(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":483},"executionInfo":{"elapsed":496,"status":"ok","timestamp":1686980973209,"user":{"displayName":"ANKEM HEMA SRIKAR / 학생 / 항공우주공학과 ­","userId":"08215093842889631142"},"user_tz":-540},"id":"7hgHngxvI8KN","outputId":"40c92bbe-1674-4e4a-eac7-ed9e2f44e3d2"},"outputs":[],"source":["class ThrowingEnvironment(ThrowingEnvironmentBase):\n","    def __init__(self, student_id=\"0000-00000\", q_size=[10, 10, 10, 10]):\n","        super().__init__(student_id=student_id, q_size=q_size)\n","\n","        # Reinforcement Learning parameters\n","        self.alpha = 0.1\n","        self.gamma = 0.99\n","        self.epsilon = 0.1\n","\n","    def action_sample_idx(self):\n","        # sample random action index\n","        action_idx = np.random.randint(len(self.action_list))\n","        return action_idx\n","\n","    def epsilon_greedy_action_idx(self, state_idx):\n","       \n","        if np.random.rand() < self.epsilon:\n","            action_idx = self.action_sample_idx()  # sample random action index\n","        else:\n","            # Choose the action with the highest Q-value for the current state\n","            action_idx = np.argmax(self.q_table[state_idx])\n","\n","        return action_idx\n","\n","    def train_step(self):\n","        \n","        self.step += 1\n","        state_idx = self.discretize(self.state)\n","        action_idx = self.epsilon_greedy_action_idx(state_idx)\n","        action = self.action_list[action_idx]\n","        next_state, reward, _, terminal = self.next_state(action)\n","        next_state_idx = self.discretize(next_state)\n","        if action[2] == 1:  # When the trash is released\n","            self.q_table[state_idx][action_idx] = reward\n","        else:\n","            # Update Q-value using the Q-learning update rule\n","            self.q_table[state_idx][action_idx] += self.alpha * (\n","                reward + self.gamma * np.max(self.q_table[next_state_idx]) - self.q_table[state_idx][action_idx]\n","            )\n","        return terminal\n","\n","discretize = [10, 10, 10, 10]\n","MODEL = ThrowingEnvironment(student_id, q_size=discretize)\n","MODEL.plot_state()"]},{"cell_type":"markdown","metadata":{"id":"fUZ40i6K1p0D"},"source":["#Now, let's train the model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":731773,"status":"ok","timestamp":1686983353882,"user":{"displayName":"ANKEM HEMA SRIKAR / 학생 / 항공우주공학과 ­","userId":"08215093842889631142"},"user_tz":-540},"id":"ZaI5n8nzI8KO"},"outputs":[],"source":["# train the model\n","train_steps = 1000000\n","MODEL.train(train_steps)\n","time.sleep(0.2)\n","\n","# save the model\n","MODEL.submit()"]},{"cell_type":"markdown","metadata":{"id":"9pC23TNo12x6"},"source":["#Let's check if the model has been trained well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XiaaDkZI8KO"},"outputs":[],"source":["# test your trained model\n","MODEL.reset([-PI/12, PI/6, 0, 0])\n","MODEL.plot_simulate(5, reset = False, is_display=True, save_mp4=True, save_name='simulation')"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
